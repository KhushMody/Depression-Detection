{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "77fb5f82",
      "metadata": {
        "id": "77fb5f82"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import functools\n",
        "from sklearn.metrics import accuracy_score \n",
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e4625a68",
      "metadata": {
        "id": "e4625a68",
        "outputId": "2e9d2f0c-f763-4397-9ccc-ae865ea2bcee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.0.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "READING THE DATA"
      ],
      "metadata": {
        "id": "2idarzhqs83Y"
      },
      "id": "2idarzhqs83Y"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b294b01a",
      "metadata": {
        "id": "b294b01a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "5a4b4e0f-945a-4373-dc07-d4762cbff4cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-15018a8d7b55>:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df = pd.read_csv(\"depression_dataset_reddit_cleaned.csv\",header=0, sep=',',warn_bad_lines=False, error_bad_lines=False)\n",
            "<ipython-input-5-15018a8d7b55>:3: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df = pd.read_csv(\"depression_dataset_reddit_cleaned.csv\",header=0, sep=',',warn_bad_lines=False, error_bad_lines=False)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             content\n",
              "0                      Don’t worry, it’s just a prop\n",
              "1   It never should have happened but that’s exac...\n",
              "2  Like really, it’s the depressed kids you’re go...\n",
              "3  Hey there, if you're feeling a bit lost and ov...\n",
              "4    no. they are meant to spur demand after elon..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be155b5f-1cb3-4126-9233-3c2d33011301\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Don’t worry, it’s just a prop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It never should have happened but that’s exac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Like really, it’s the depressed kids you’re go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hey there, if you're feeling a bit lost and ov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>no. they are meant to spur demand after elon...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be155b5f-1cb3-4126-9233-3c2d33011301')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-be155b5f-1cb3-4126-9233-3c2d33011301 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-be155b5f-1cb3-4126-9233-3c2d33011301');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"depression_dataset_reddit_cleaned.csv\",header=0, sep=',',warn_bad_lines=False, error_bad_lines=False)\n",
        "df_test = pd.read_csv('/content/merged_dataset.csv')\n",
        "df_test = df_test.drop(['Unnamed: 0'], axis=1)\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "zb2ut3ZnmoKx",
        "outputId": "727ee169-67c7-42e6-e0ec-e6b12ffcfd3d"
      },
      "id": "zb2ut3ZnmoKx",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             clean_text  is_depression\n",
              "0     we understand that most people who reply immed...              1\n",
              "1     welcome to r depression s check in post a plac...              1\n",
              "2     anyone else instead of sleeping more when depr...              1\n",
              "3     i ve kind of stuffed around a lot in my life d...              1\n",
              "4     sleep is my greatest and most comforting escap...              1\n",
              "...                                                 ...            ...\n",
              "7726                                       is that snow              0\n",
              "7727                 moulin rouge mad me cry once again              0\n",
              "7728  trying to shout but can t find people on the list              0\n",
              "7729  ughh can t find my red sox hat got ta wear thi...              0\n",
              "7730  slept wonderfully finally tried swatching for ...              0\n",
              "\n",
              "[7731 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-50787331-40b0-4e81-a33f-97431450ab54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "      <th>is_depression</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>we understand that most people who reply immed...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>welcome to r depression s check in post a plac...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anyone else instead of sleeping more when depr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i ve kind of stuffed around a lot in my life d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sleep is my greatest and most comforting escap...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7726</th>\n",
              "      <td>is that snow</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7727</th>\n",
              "      <td>moulin rouge mad me cry once again</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7728</th>\n",
              "      <td>trying to shout but can t find people on the list</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7729</th>\n",
              "      <td>ughh can t find my red sox hat got ta wear thi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7730</th>\n",
              "      <td>slept wonderfully finally tried swatching for ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7731 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50787331-40b0-4e81-a33f-97431450ab54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-50787331-40b0-4e81-a33f-97431450ab54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-50787331-40b0-4e81-a33f-97431450ab54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA CLEANING AND PREPROCESSING"
      ],
      "metadata": {
        "id": "vqxdN7JJtArK"
      },
      "id": "vqxdN7JJtArK"
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_text'] = df['clean_text'].str.replace(r'<[^<>]*>', '', regex=True)#1\n",
        "df['clean_text'] = df['clean_text'].str.replace(r'http://\\S+|https://\\S+', '', regex=True)#2\n",
        "df['clean_text']=df['clean_text'].str.lower()\n",
        "df['clean_text'] = df['clean_text'].str.replace(r'[@_!#$%^&*()<>?/\\|}{~:]', '', regex=True)#5\n",
        "df['clean_text'] = df['clean_text'].replace(\"\\s+\", \" \", regex=True).str.strip()#last\n",
        "after_dc=df['clean_text'].str.len().mean()\n",
        "df_test['clean_text'] = df_test['content'].str.replace(r'<[^<>]*>', '', regex=True)#1\n",
        "df_test['clean_text'] = df_test['clean_text'].str.replace(r'http://\\S+|https://\\S+', '', regex=True)#2\n",
        "df_test['clean_text']=df_test['clean_text'].str.lower()\n",
        "df_test['clean_text'] = df_test['clean_text'].str.replace(r'[@_!#$%^&*()<>?/\\|}{~:]', '', regex=True)#5\n",
        "df_test['clean_text'] = df_test['clean_text'].replace(\"\\s+\", \" \", regex=True).str.strip()#last\n",
        "#after_dc=df['clean_text'].str.len().mean()\n",
        "\n",
        "#DATA PREPOCESSING\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    #tokenize the sentence and find the POS tag for each token\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "    #tuple of (token, wordnet_tag)\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:\n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "    return \" \".join(lemmatized_sentence)\n",
        "\n",
        "\n",
        "\n",
        "# Lemmatizing\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: lemmatize_sentence(x))\n",
        "df_test['clean_text'] = df_test['clean_text'].apply(lambda x: lemmatize_sentence(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwWLaLYLmVm2",
        "outputId": "816eef5f-e9cf-4de1-b631-416b01b01378"
      },
      "id": "bwWLaLYLmVm2",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df_test.drop(['content'], axis = 1)\n",
        "df_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "115tYA8i2quQ",
        "outputId": "88ac80c8-636e-4d29-cc41-447ee8860755"
      },
      "id": "115tYA8i2quQ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          clean_text\n",
              "0                 don ’ t worry , it ’ s just a prop\n",
              "1  it never should have happen but that ’ s exact...\n",
              "2  like really , it ’ s the depressed kid you ’ r...\n",
              "3  hey there , if you 're feel a bit lose and ove...\n",
              "4  no . they be mean to spur demand after elon bo..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48872caa-001a-4713-bd5b-3d07f6802be6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>don ’ t worry , it ’ s just a prop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it never should have happen but that ’ s exact...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>like really , it ’ s the depressed kid you ’ r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hey there , if you 're feel a bit lose and ove...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>no . they be mean to spur demand after elon bo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48872caa-001a-4713-bd5b-3d07f6802be6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-48872caa-001a-4713-bd5b-3d07f6802be6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-48872caa-001a-4713-bd5b-3d07f6802be6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORD2VEC MODEL"
      ],
      "metadata": {
        "id": "LN9XHohVtF2Z"
      },
      "id": "LN9XHohVtF2Z"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6fb2a566",
      "metadata": {
        "id": "6fb2a566",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb62bfb-47d0-447e-876f-773fcce9697e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "#The pretrained googleW2V model is loaded to generate the word embeddings\n",
        "import gensim.downloader as googleW2V\n",
        "google_w2v = googleW2V.load(\"word2vec-google-news-300\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPLITTING DATA INTO TRAIN AND TEST"
      ],
      "metadata": {
        "id": "sF_lVprGtJue"
      },
      "id": "sF_lVprGtJue"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d04dd80e",
      "metadata": {
        "id": "d04dd80e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d82ae5c2-d34e-41b8-9842-2aa0b95a056e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   don ’ t worry , it ’ s just a prop\n",
              "1    it never should have happen but that ’ s exact...\n",
              "2    like really , it ’ s the depressed kid you ’ r...\n",
              "3    hey there , if you 're feel a bit lose and ove...\n",
              "4    no . they be mean to spur demand after elon bo...\n",
              "Name: clean_text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "X=df['clean_text']\n",
        "y=df['is_depression']\n",
        "\n",
        "X_unlabeled = df_test['clean_text']\n",
        "X_unlabeled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "99bd9e04",
      "metadata": {
        "id": "99bd9e04"
      },
      "outputs": [],
      "source": [
        "#dataset is split into training and testing sets and the indexes are reset \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fa073ef3",
      "metadata": {
        "id": "fa073ef3"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3e3a534c",
      "metadata": {
        "id": "3e3a534c"
      },
      "outputs": [],
      "source": [
        "X_test = X_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bc6a389a",
      "metadata": {
        "id": "bc6a389a"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3887fc2c",
      "metadata": {
        "id": "3887fc2c"
      },
      "outputs": [],
      "source": [
        "y_test = y_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AVERAGE OF THE VECTORS"
      ],
      "metadata": {
        "id": "L7pGI-05tOxr"
      },
      "id": "L7pGI-05tOxr"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a63fc028",
      "metadata": {
        "id": "a63fc028",
        "outputId": "c06d6b3e-10bc-4d8d-8ca9-80a575de95c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.9/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ],
      "source": [
        "#The average is found between all the vectors corresponding to the words in a sentence.\n",
        "#Average found and appended for each sentence for the training set\n",
        "X_train1 = []\n",
        "X_unlabeled1 = []\n",
        "\n",
        "for x in X_train:\n",
        "    wordveclist = []\n",
        "    for word in x.split(' '):\n",
        "        try:\n",
        "            wordvec = google_w2v[word]\n",
        "            wordveclist.append(wordvec)\n",
        "        except:\n",
        "            pass\n",
        "    X_train1.append(np.mean(wordveclist, axis=0))\n",
        "\n",
        "for x in X_unlabeled:\n",
        "    wordveclist = []\n",
        "    for word in x.split(' '):\n",
        "        try:\n",
        "            wordvec = google_w2v[word]\n",
        "            wordveclist.append(wordvec)\n",
        "        except:\n",
        "            pass\n",
        "    X_unlabeled1.append(np.mean(wordveclist, axis=0)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8d21fc6f",
      "metadata": {
        "id": "8d21fc6f"
      },
      "outputs": [],
      "source": [
        "#Average found and appended for each sentence for the test set\n",
        "X_test1 = []\n",
        "for x in X_test:\n",
        "    wordveclist = []\n",
        "    for word in x.split(' '):\n",
        "        try:\n",
        "            wordvec = google_w2v[word]\n",
        "            wordveclist.append(wordvec)\n",
        "        except:\n",
        "            pass\n",
        "    X_test1.append(np.mean(wordveclist, axis=0)) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_unlabeled1[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO1GYBJr47zA",
        "outputId": "eccb58f4-f1da-465a-d655-575df4fe8ddb"
      },
      "id": "dO1GYBJr47zA",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-2.40652896e-02,  7.53672495e-02, -8.52399543e-02,  1.20186940e-01,\n",
              "        -9.16748047e-02,  1.88337057e-03,  2.46276855e-02, -6.94754496e-02,\n",
              "         1.25313893e-01, -7.44628906e-03, -5.99670410e-03, -1.12409316e-01,\n",
              "        -6.56999871e-02,  2.71944311e-02, -1.67445585e-01,  1.47251680e-01,\n",
              "         9.30960551e-02,  1.55587330e-01, -4.85316701e-02, -1.27790183e-01,\n",
              "        -9.62960348e-02,  3.05873323e-02,  1.12670898e-01, -1.95922852e-02,\n",
              "         8.67396742e-02,  4.01349738e-02, -7.62416273e-02, -4.43551205e-02,\n",
              "         2.16500424e-02, -4.72935252e-02, -1.13568986e-02,  1.09165736e-01,\n",
              "        -8.04530531e-02,  2.85382960e-02,  1.74386165e-04,  4.23060842e-02,\n",
              "        -5.72596975e-02,  1.14593506e-01,  2.10658479e-02,  1.45019531e-01,\n",
              "         1.32108415e-02, -1.01087295e-01,  1.89383373e-01, -4.29240651e-02,\n",
              "        -8.00868403e-03,  2.82854345e-02, -7.32596293e-02, -9.91249084e-03,\n",
              "         2.83028744e-02,  1.35881692e-01, -7.29806051e-02,  1.34717673e-01,\n",
              "         4.11791131e-02,  1.01824082e-01,  1.14519395e-01, -5.25425486e-02,\n",
              "        -3.68941166e-02, -9.36453715e-02, -7.41141208e-04, -1.15855627e-01,\n",
              "        -8.31647590e-02,  1.14048552e-02, -4.97741699e-02, -9.34317484e-02,\n",
              "         3.95464227e-02, -4.49218750e-02, -1.50348112e-01,  5.53065725e-02,\n",
              "        -3.15987729e-02,  8.47691149e-02,  9.32617188e-02,  4.85665463e-02,\n",
              "        -4.16782917e-03, -4.63169627e-02, -9.86066535e-02, -2.11661197e-02,\n",
              "         1.07474193e-01,  8.29554945e-02,  3.13916877e-02,  5.96226268e-02,\n",
              "        -1.56947542e-02, -7.15855211e-02, -2.60184146e-02,  2.50374936e-02,\n",
              "        -2.46930812e-02,  2.30712891e-02, -7.99909309e-02,  1.23168945e-01,\n",
              "        -5.06504588e-02, -4.28989949e-03,  2.56985258e-02,  1.01911269e-01,\n",
              "        -6.80062398e-02, -1.34416848e-01,  4.59333137e-02, -7.09402934e-02,\n",
              "         1.56947542e-02,  5.13785221e-02,  9.90164652e-02, -4.26374152e-02,\n",
              "        -5.04602715e-02, -1.75432470e-02, -4.37186100e-02, -3.94112729e-02,\n",
              "        -3.43889520e-02,  2.19770167e-02,  2.68729068e-02,  4.77818074e-03,\n",
              "         2.03813817e-02,  2.00195312e-02, -8.02001953e-02, -8.97478387e-02,\n",
              "         3.49579863e-02,  1.93198062e-02,  7.93021098e-02,  3.41796875e-02,\n",
              "         5.86896613e-02, -6.79103285e-02, -8.92857183e-03,  7.77064711e-02,\n",
              "        -5.96400676e-03,  7.91364387e-02, -8.27985480e-02,  9.99232680e-02,\n",
              "         1.86941959e-02, -1.08607702e-01, -6.63452148e-02, -4.17567678e-02,\n",
              "        -1.30310059e-02, -1.26985824e-02, -8.97739977e-02, -6.87452033e-02,\n",
              "        -9.77870356e-03,  3.41099314e-02, -5.70068359e-02, -2.04696655e-02,\n",
              "         9.88333579e-03,  9.49358270e-02, -3.29589844e-03,  4.53404002e-02,\n",
              "         1.56912670e-01, -1.76740378e-01,  1.25191823e-01,  1.34277344e-02,\n",
              "         4.95082326e-02,  9.94001143e-03, -2.34200619e-02,  2.85818912e-02,\n",
              "        -2.05775676e-03, -8.02873895e-02, -2.05775676e-03, -1.37834817e-01,\n",
              "        -5.04324771e-02,  6.01196289e-03, -5.16008660e-02, -1.45045687e-02,\n",
              "        -8.93641859e-02, -1.65318083e-02, -7.75669664e-02, -3.43889520e-02,\n",
              "         5.13218455e-02,  2.79017854e-02,  1.26299178e-02, -3.37655209e-02,\n",
              "         6.70950767e-03, -1.46798268e-01,  1.71212330e-01,  2.69208639e-03,\n",
              "         5.30744269e-02,  3.33949514e-02, -1.69834688e-01,  1.08686173e-02,\n",
              "        -3.25927734e-02, -2.51987996e-03,  3.89338918e-02, -1.60609651e-02,\n",
              "         1.16768971e-01, -1.40154153e-01, -2.45535709e-02, -3.14592645e-02,\n",
              "         8.10895674e-03, -8.21334273e-02,  8.11179057e-02, -2.84772608e-02,\n",
              "         4.19965461e-02, -4.60728221e-02, -2.09786557e-02,  7.39746094e-02,\n",
              "         9.08203125e-02,  5.02232127e-02,  9.68889520e-02, -4.95256716e-03,\n",
              "        -3.91932912e-02,  3.44194695e-02,  3.39355469e-02, -5.85414357e-02,\n",
              "        -5.07463738e-02,  5.25251105e-02, -3.83649545e-04, -1.06305800e-01,\n",
              "        -1.31661547e-02,  3.66634652e-02, -1.43484935e-01, -3.69873047e-02,\n",
              "         1.02556504e-01,  3.39355469e-02, -1.90429688e-02, -3.81033751e-03,\n",
              "        -2.64413026e-03, -8.01217183e-02,  4.67529297e-02,  1.69573098e-01,\n",
              "        -2.51464844e-02, -7.81250000e-03, -1.24476843e-01,  3.92717645e-02,\n",
              "         9.13783461e-02, -1.54680520e-01, -1.59249440e-01,  5.45479916e-02,\n",
              "        -8.71930793e-02,  1.70200896e-02, -4.35267873e-02, -1.95486881e-02,\n",
              "         5.82798533e-02,  2.68554688e-02,  1.28696989e-02, -3.10952333e-03,\n",
              "         3.55747752e-02, -4.25720215e-02,  2.71671843e-02, -9.65401754e-02,\n",
              "         1.03991374e-03, -1.62004735e-02,  5.58384471e-02,  3.61755900e-02,\n",
              "        -1.15487231e-02, -1.19419642e-01, -7.99124595e-03,  1.12827849e-02,\n",
              "         5.28782420e-02,  7.08007812e-03,  6.49414062e-02, -5.67278191e-02,\n",
              "         2.42135189e-02,  5.86286262e-02, -4.06494141e-02, -1.10909594e-02,\n",
              "         3.70246358e-02, -6.40280619e-02, -2.78690886e-02, -3.83878425e-02,\n",
              "         1.35018483e-01,  7.34122172e-02,  1.68457031e-02, -5.50188357e-03,\n",
              "         6.78275004e-02,  1.19001113e-01, -6.49152473e-02, -4.60379478e-03,\n",
              "         4.21840139e-02,  1.00847520e-01, -3.13895084e-02,  6.65501179e-03,\n",
              "         4.09458689e-02,  5.84640503e-02,  9.36453696e-03,  3.25709768e-02,\n",
              "        -1.22942245e-02, -7.77021125e-02, -1.48577010e-02,  8.01130012e-02,\n",
              "         2.31236055e-01, -1.74080990e-02,  8.96432027e-02, -1.34373263e-01,\n",
              "        -1.05991907e-01, -1.59319192e-01, -2.93666292e-02, -1.68108251e-02,\n",
              "        -6.72738180e-02,  7.62939453e-02,  1.13865443e-01,  2.67159604e-02,\n",
              "         4.56957147e-02,  6.88486099e-02, -6.98503777e-02, -9.39069502e-03,\n",
              "         3.04303858e-02, -9.58426371e-02, -1.64969303e-02,  6.77141473e-02,\n",
              "        -3.68259959e-02,  9.23461914e-02, -5.36869578e-02, -1.24167308e-01,\n",
              "        -1.77525114e-02, -9.88943949e-02,  8.14732164e-02,  3.13023143e-02],\n",
              "       dtype=float32),\n",
              " array([-0.00947043,  0.01441537,  0.01347938,  0.09877659, -0.04351103,\n",
              "        -0.01519775,  0.02803979, -0.08205003,  0.08112863,  0.06897442,\n",
              "        -0.07241352, -0.15058312, -0.05413407, -0.00339215, -0.07843487,\n",
              "         0.06364441,  0.06146768,  0.06492849, -0.00706658, -0.08624737,\n",
              "        -0.02097636,  0.0720567 ,  0.08702909, -0.04790321,  0.01007667,\n",
              "         0.00024766, -0.09981947,  0.0383113 ,  0.05638768, -0.00461754,\n",
              "        -0.02712599,  0.03869864, -0.06696261,  0.0094804 ,  0.06565035,\n",
              "        -0.00648117,  0.02800809, -0.03346487,  0.05276489,  0.07405222,\n",
              "         0.09004094, -0.00659414,  0.09695141, -0.05718525, -0.01229976,\n",
              "        -0.03694094, -0.0204996 ,  0.01877418,  0.02222413,  0.07416006,\n",
              "        -0.03760705,  0.03350478, -0.01327045,  0.00452599,  0.03010676,\n",
              "         0.07789817, -0.01508918, -0.03895334,  0.01002326, -0.0957178 ,\n",
              "        -0.02585308,  0.10647289, -0.05151837, -0.03769625, -0.03403414,\n",
              "        -0.07170457, -0.00491128,  0.06253052, -0.07802758,  0.07770479,\n",
              "         0.06066895,  0.05603027,  0.05371328, -0.0117525 , -0.09078275,\n",
              "        -0.0274893 ,  0.09144475,  0.04781283,  0.03523372,  0.08315922,\n",
              "         0.00761472, -0.0626655 ,  0.05887853, -0.02189196, -0.07123155,\n",
              "        -0.07616249, -0.11272005,  0.06494229, -0.00464073, -0.02455961,\n",
              "         0.03539746,  0.04026208, -0.01558216, -0.10517766, -0.00728901,\n",
              "        -0.04185046,  0.02754839,  0.05575327,  0.01337286, -0.0363347 ,\n",
              "        -0.05679402, -0.04078088,  0.03916344,  0.01676119, -0.01665321,\n",
              "        -0.02537419, -0.04064237, -0.03885709,  0.00620915, -0.04590313,\n",
              "        -0.06329933, -0.01170819, -0.0257404 ,  0.00951679,  0.05296678,\n",
              "        -0.02295743,  0.08135157, -0.04877941,  0.06734995,  0.03008329,\n",
              "        -0.09088135, -0.00525372, -0.07931519,  0.07547701, -0.05852802,\n",
              "        -0.04549584, -0.05090024, -0.02780914,  0.02269158, -0.00805283,\n",
              "        -0.04163771, -0.07463015, -0.08695162, -0.05581665, -0.01855645,\n",
              "        -0.05711482, -0.0120951 , -0.00596736,  0.00958252,  0.08780142,\n",
              "         0.05968446, -0.04040997,  0.04774534, -0.0194397 , -0.00572205,\n",
              "         0.00331527, -0.04323637, -0.07809801, -0.05044086, -0.00782424,\n",
              "         0.06024053,  0.03854605, -0.12467135,  0.0105403 , -0.00275773,\n",
              "        -0.00843694, -0.05272557, -0.05597159, -0.02951519, -0.01493689,\n",
              "         0.02427673,  0.04926946,  0.04767257,  0.00526546,  0.03124648,\n",
              "        -0.1106004 , -0.00103547, -0.04265184, -0.00155581,  0.02574745,\n",
              "        -0.09088604, -0.03448721, -0.06379465, -0.05090244, -0.03212562,\n",
              "         0.00942142,  0.07335487, -0.0792861 ,  0.06371161, -0.02326848,\n",
              "        -0.06234741, -0.07184242,  0.05330071,  0.02663715, -0.02678152,\n",
              "        -0.02214168, -0.05258648,  0.01627819,  0.07494648,  0.05596748,\n",
              "         0.03363213,  0.03590745,  0.03700139,  0.02275555, -0.01029381,\n",
              "        -0.03343083, -0.04198573,  0.01229741, -0.04947603, -0.13548866,\n",
              "        -0.01607337, -0.00026468, -0.06536751, -0.02058909,  0.02356896,\n",
              "        -0.02324207, -0.01984875,  0.00954496,  0.04764263,  0.00054213,\n",
              "        -0.01117765,  0.00383348, -0.03078285,  0.01610859, -0.1570153 ,\n",
              "         0.03352062,  0.12289076, -0.03442383, -0.09100048,  0.02647517,\n",
              "        -0.00693453,  0.0130134 ,  0.00608943, -0.0095074 ,  0.09125225,\n",
              "        -0.03578538,  0.09736986,  0.0480065 ,  0.04799241, -0.01907642,\n",
              "         0.02005122, -0.0437493 , -0.01292592,  0.02027403,  0.04150567,\n",
              "        -0.00391447, -0.0172618 , -0.04045692,  0.08015089, -0.02693382,\n",
              "         0.01557218, -0.03236609,  0.06242723, -0.07301991, -0.00362909,\n",
              "         0.02121676,  0.06403057,  0.03220074, -0.01365126, -0.02877866,\n",
              "         0.0015071 ,  0.05351375,  0.05615938,  0.04511496,  0.06819388,\n",
              "        -0.0213089 ,  0.03325242,  0.05249023, -0.06608464, -0.08530609,\n",
              "         0.00191439,  0.00221487, -0.03997524,  0.06019592,  0.04293471,\n",
              "         0.12888043, -0.0122176 ,  0.01407565, -0.03484755, -0.02564063,\n",
              "         0.05896231,  0.05897053,  0.11429552,  0.08611356, -0.00152588,\n",
              "        -0.07335193, -0.03509521, -0.10056715, -0.02324412, -0.04256733,\n",
              "         0.016287  ,  0.02564122,  0.04898775,  0.07678751,  0.05589177,\n",
              "        -0.03747764, -0.04236999, -0.0200923 , -0.00019484,  0.02222971,\n",
              "        -0.04183021,  0.0519327 , -0.06596022,  0.0471567 , -0.03150515,\n",
              "        -0.03334207, -0.02914194, -0.07152382,  0.03667274, -0.05252721],\n",
              "       dtype=float32),\n",
              " array([ 0.04365072,  0.04937148,  0.03439207,  0.13766916, -0.0714659 ,\n",
              "         0.00087069,  0.03228137, -0.05807308,  0.02831782,  0.03919769,\n",
              "        -0.04543538, -0.10701207, -0.02961949, -0.00280419, -0.13698904,\n",
              "         0.04874856,  0.04728169,  0.10490293,  0.01946399, -0.07023045,\n",
              "        -0.03371414,  0.0404825 ,  0.070038  , -0.02002919,  0.05597379,\n",
              "        -0.00945506, -0.04805515,  0.0448216 ,  0.04682424, -0.03983758,\n",
              "        -0.02380402,  0.08171206, -0.06927241, -0.02911206,  0.00068011,\n",
              "         0.01794916,  0.06458169, -0.01149267,  0.00502108,  0.13298003,\n",
              "         0.06844657, -0.07940549,  0.14897327, -0.04077133,  0.0191409 ,\n",
              "        -0.02793043, -0.00051292,  0.0058373 ,  0.01677112,  0.03971053,\n",
              "        -0.04707804,  0.10996916,  0.00425876, -0.01178228,  0.01771452,\n",
              "         0.01082969, -0.02619405, -0.0339779 ,  0.04731175, -0.04145813,\n",
              "        -0.00239501,  0.05928507, -0.07858463, -0.03291352, -0.00264382,\n",
              "        -0.0526826 , -0.04682533,  0.09383377, -0.0199381 ,  0.07065941,\n",
              "         0.09257807,  0.05171844,  0.04785654, -0.0075946 , -0.14585379,\n",
              "        -0.03620506,  0.0921964 ,  0.10091229,  0.05980122,  0.13161235,\n",
              "         0.02738081, -0.06054999,  0.03096787,  0.01662959, -0.08286114,\n",
              "        -0.00738276, -0.04218027,  0.14956291,  0.02279383, -0.00975192,\n",
              "         0.05269895,  0.0782835 , -0.07715218, -0.12320382,  0.00340396,\n",
              "        -0.06637885,  0.09232315,  0.07663711,  0.00791028, -0.0193715 ,\n",
              "        -0.06445383, -0.06415971,  0.01872222,  0.02953728, -0.06691633,\n",
              "        -0.00989525, -0.01907535, -0.06010966,  0.01270621, -0.04421654,\n",
              "        -0.08069346, -0.02034775, -0.04743707, -0.04720696,  0.09267379,\n",
              "        -0.00226889,  0.03109554, -0.08546012,  0.06679278,  0.04243703,\n",
              "        -0.06758694,  0.01427974, -0.09395459,  0.08226932, -0.01443536,\n",
              "        -0.02379157, -0.03396046, -0.05086128, -0.00232815, -0.01321146,\n",
              "        -0.07476869, -0.10990843, -0.06907   ,  0.0348598 , -0.01015301,\n",
              "        -0.03473609,  0.02724853,  0.01459238,  0.0341554 ,  0.09357943,\n",
              "         0.09459079, -0.09193016,  0.04310514,  0.00345596,  0.00172393,\n",
              "         0.0629091 , -0.00921911, -0.07063823, -0.03382718, -0.03387015,\n",
              "         0.06741987,  0.01035418, -0.09259936,  0.03666344, -0.03032373,\n",
              "        -0.0094311 , -0.04713845, -0.0540267 , -0.04734974, -0.00578028,\n",
              "         0.0317115 ,  0.05595304,  0.02321142,  0.07708647,  0.03267   ,\n",
              "        -0.10845356,  0.0835857 , -0.04142761,  0.04655363, -0.00842877,\n",
              "        -0.13173006, -0.04521771, -0.02286078, -0.07092939, -0.04480012,\n",
              "         0.01236009,  0.12027352, -0.03714779, -0.03837118,  0.00795793,\n",
              "        -0.0519431 , -0.05562008,  0.03207896,  0.03111423, -0.03647162,\n",
              "        -0.04079141, -0.01634045,  0.05296668,  0.07444701,  0.05023832,\n",
              "         0.03332488,  0.04516134,  0.01689304,  0.00058482,  0.0376562 ,\n",
              "         0.00535739, -0.00739491, -0.01050178, -0.07701983, -0.11374213,\n",
              "         0.0154802 ,  0.01150215, -0.07761679, -0.01812954,  0.01914168,\n",
              "        -0.01434575, -0.04236961, -0.01129228,  0.02472578, -0.0284673 ,\n",
              "         0.00215741,  0.09611324, -0.02823935,  0.02505618, -0.13087869,\n",
              "        -0.00565509,  0.10200438, -0.02280815, -0.11898492, -0.02602402,\n",
              "        -0.03356996,  0.01935313, -0.02413318, -0.03119893,  0.08240182,\n",
              "        -0.05144781,  0.0903993 ,  0.01253027, -0.02014783, -0.01989155,\n",
              "         0.01844103, -0.05999943, -0.01992195,  0.0271786 ,  0.06859697,\n",
              "        -0.00030156, -0.04761801, -0.06438835,  0.08711461,  0.03377175,\n",
              "         0.054368  , -0.01382314, -0.00116418, -0.0627896 , -0.03629474,\n",
              "         0.06412553, -0.01598872,  0.05009305, -0.00127064, -0.04886814,\n",
              "         0.03530167,  0.00238574,  0.08128388,  0.05043061,  0.05384982,\n",
              "        -0.02865655,  0.02754118,  0.0396442 , -0.06538484, -0.06652458,\n",
              "        -0.00019307, -0.02755768, -0.08880366,  0.02555209,  0.05308501,\n",
              "         0.11466264,  0.02026834,  0.02195459, -0.08870526,  0.00462668,\n",
              "         0.05598715,  0.12290192,  0.14776237,  0.04680338,  0.02983868,\n",
              "        -0.07609309, -0.02859559, -0.10722382, -0.04350686, -0.02456653,\n",
              "         0.01772853, -0.03654324,  0.02627813,  0.08825964,  0.010281  ,\n",
              "         0.00620083, -0.10658046, -0.05734969,  0.01685723,  0.05684304,\n",
              "        -0.02637077,  0.07740939, -0.10282805, -0.00822418, -0.04466761,\n",
              "        -0.05402479,  0.03522398, -0.07307683,  0.02283275, -0.01158422],\n",
              "       dtype=float32),\n",
              " array([ 3.97896618e-02,  3.69536318e-02,  5.41502843e-03,  1.18471339e-01,\n",
              "        -8.32245946e-02,  3.23444232e-02,  4.40915860e-02, -6.28272742e-02,\n",
              "         4.17377194e-03,  8.65302235e-02, -4.55574840e-02, -1.34412035e-01,\n",
              "        -2.44337283e-02,  1.00821136e-02, -1.03039972e-01,  1.07169315e-01,\n",
              "         7.83417821e-02,  1.29255623e-01,  4.52081077e-02, -7.40790516e-02,\n",
              "        -4.30297852e-02,  3.68231423e-02,  1.23916097e-01, -2.88506877e-02,\n",
              "         7.96271339e-02,  4.33865264e-02, -7.67672285e-02, -2.47765910e-02,\n",
              "         5.23755290e-02, -2.80088224e-02, -5.52683845e-02,  7.88879395e-02,\n",
              "        -3.51373069e-02, -6.28620014e-02, -8.23553652e-03, -3.95657755e-02,\n",
              "         3.89972553e-02,  1.30278489e-03,  5.47979958e-02,  1.07617609e-01,\n",
              "         1.13068677e-01, -8.00339282e-02,  1.81093410e-01, -2.59820349e-03,\n",
              "        -5.06370813e-02, -1.22883897e-02, -3.87531146e-02, -7.86722079e-03,\n",
              "         2.84618512e-02,  5.89323379e-02,  3.35656516e-02,  1.14830278e-01,\n",
              "        -4.87865582e-02, -8.14082660e-03,  3.23381089e-02, -4.39942442e-02,\n",
              "        -1.13733225e-02, -7.76809156e-02,  4.22531664e-02, -9.35121700e-02,\n",
              "        -3.56003339e-03,  4.64482792e-02, -6.28798902e-02, -9.09518525e-02,\n",
              "         1.68267619e-02, -6.35996833e-02, -8.93817768e-02,  8.71603042e-02,\n",
              "        -1.03155069e-01,  6.73138872e-02,  4.74443100e-02,  7.47196600e-02,\n",
              "         6.07310310e-02, -4.09314372e-02, -1.99952230e-01, -1.08052224e-01,\n",
              "         5.98660186e-02,  8.23869407e-02,  2.36774322e-05,  1.50165945e-01,\n",
              "         9.45939682e-03, -8.85393843e-02,  7.22298473e-02, -1.61217004e-02,\n",
              "        -3.62170003e-02, -4.32528779e-02, -6.58427253e-02,  1.53145626e-01,\n",
              "         2.13149507e-02,  1.90511905e-02,  6.22779578e-02,  5.69563247e-02,\n",
              "        -8.82715657e-02, -8.27342048e-02, -2.34332904e-02, -7.89952800e-02,\n",
              "         9.69953835e-02,  9.50759351e-02, -6.79871114e-03, -1.35329673e-02,\n",
              "        -7.26974756e-02, -3.68265621e-02,  2.39637960e-02,  1.02758750e-01,\n",
              "        -1.08735181e-01, -5.66448346e-02, -1.07554466e-01, -5.83243519e-02,\n",
              "         4.01595533e-02, -5.83190918e-02, -3.52109708e-02, -3.64253595e-02,\n",
              "         2.36142916e-03, -2.85139419e-02,  1.10052703e-02, -4.01586331e-02,\n",
              "         4.19500954e-02, -4.23962809e-02,  6.53797016e-02,  4.11261208e-02,\n",
              "        -7.37704560e-02, -5.62444236e-03, -3.30242291e-02,  1.25523135e-01,\n",
              "        -2.00321600e-02,  5.43791684e-04, -8.53881836e-02, -4.12334576e-02,\n",
              "        -2.45482335e-03,  3.72514389e-02, -5.42050079e-02, -1.35333881e-01,\n",
              "        -9.98261571e-02, -2.86823139e-02, -7.75777875e-03, -1.18441880e-01,\n",
              "         4.94708046e-02,  9.13527906e-02, -5.77939767e-03,  1.02198109e-01,\n",
              "         3.50836404e-02, -1.00219727e-01,  3.44701298e-02, -2.85718199e-02,\n",
              "         4.44630608e-02, -8.24290328e-03, -4.21563499e-02, -1.51642904e-01,\n",
              "        -5.10338098e-02, -5.63464984e-02,  5.38585261e-02,  5.26817590e-02,\n",
              "        -1.77203998e-01,  5.04852831e-02, -1.19986702e-02, -2.00784616e-02,\n",
              "        -4.35769968e-02, -1.21095888e-01, -1.51316151e-01,  5.10338098e-02,\n",
              "         3.15036103e-02,  7.20125362e-02, -2.52417196e-02,  4.31255475e-02,\n",
              "         6.79195002e-02, -1.24305464e-01,  2.97660828e-02, -6.63073286e-02,\n",
              "         5.53146899e-02, -3.57887000e-02, -1.99634418e-01, -2.82171834e-02,\n",
              "        -2.75468491e-02, -7.31979907e-02, -2.87907049e-02, -2.97283307e-02,\n",
              "         5.94154894e-02, -2.82066613e-02, -4.29350766e-04,  6.26220703e-02,\n",
              "        -6.64504468e-02, -5.97807802e-02,  7.69416541e-02,  3.86097357e-02,\n",
              "        -6.55249208e-02, -3.25796194e-02, -9.83828828e-02, -2.07940466e-03,\n",
              "         6.10814579e-02,  1.91382039e-02,  3.28874253e-02,  2.73626912e-02,\n",
              "        -4.76758229e-03, -1.04201743e-02, -1.50104389e-02,  8.01612623e-03,\n",
              "        -4.72556800e-02,  2.13917699e-02, -8.03738981e-02, -9.16011408e-02,\n",
              "         3.73913981e-02,  6.02522232e-02, -6.83746338e-02,  1.69365983e-02,\n",
              "         4.27509185e-05,  3.00771780e-02, -1.11435466e-01, -8.63458067e-02,\n",
              "         1.88198742e-02,  4.52038981e-02, -8.16239975e-03,  8.74402300e-02,\n",
              "        -4.74621989e-02,  2.52496134e-02, -1.22622788e-01, -1.57623291e-02,\n",
              "         1.13125503e-01, -5.37990686e-03, -1.28864154e-01, -3.47942486e-02,\n",
              "        -6.25207871e-02, -3.36819366e-02, -5.12521677e-02, -5.47127575e-02,\n",
              "         5.95003329e-02, -6.11509122e-02,  7.07221180e-02,  9.14659165e-03,\n",
              "        -5.48127256e-02,  1.78990848e-02,  1.35371769e-02, -4.00674753e-02,\n",
              "        -4.04094841e-04,  1.51417172e-02,  7.50437751e-02,  4.14852276e-02,\n",
              "        -5.94114102e-02, -4.61467877e-02,  8.29972923e-02,  5.80595760e-03,\n",
              "         3.29053141e-02,  9.07571763e-02,  2.53369566e-02, -1.06788374e-01,\n",
              "        -5.44454642e-02,  9.19736642e-03, -1.34109631e-02,  9.62050855e-02,\n",
              "        -4.09598500e-02, -4.94174287e-03,  3.06933168e-02,  4.09595892e-02,\n",
              "         8.78332704e-02,  6.86136112e-02,  4.19753492e-02, -3.77223566e-02,\n",
              "         4.70370576e-02,  3.21918353e-02, -8.51629823e-02, -5.45633249e-02,\n",
              "        -6.57075038e-03, -5.23850024e-02, -8.40769783e-02,  7.33747780e-02,\n",
              "         1.86717585e-02,  1.12985149e-01,  8.44705477e-03, -4.70181182e-02,\n",
              "        -8.69324803e-02, -3.48137170e-02,  3.49631459e-02,  1.59716383e-01,\n",
              "         1.73129380e-01,  8.60427320e-02,  2.32228246e-02, -7.71505386e-02,\n",
              "        -7.19688684e-02, -1.99740708e-01, -5.23386970e-02, -3.51010039e-02,\n",
              "         4.02474254e-02, -6.70376495e-02,  7.91394487e-02,  3.49399969e-02,\n",
              "         2.62214406e-03,  1.47002321e-02, -1.07143007e-01, -6.90118223e-02,\n",
              "         4.55227569e-02,  6.55191392e-02, -4.17827740e-02,  1.19083010e-01,\n",
              "        -6.09720163e-02,  1.24994479e-02,  8.64752382e-03, -2.54600793e-02,\n",
              "        -1.68141332e-02, -7.45870620e-02,  4.45906557e-02, -3.84241305e-02],\n",
              "       dtype=float32),\n",
              " array([ 5.71624748e-02,  1.03782658e-02,  6.73149079e-02,  8.71765167e-02,\n",
              "        -1.05194092e-01, -3.89617905e-02,  3.02345268e-02, -6.27906770e-02,\n",
              "         7.86560029e-02,  5.07106781e-02, -7.55088776e-02, -1.12057492e-01,\n",
              "        -7.69329083e-04,  8.19976777e-02, -9.67559814e-02,  1.12121962e-01,\n",
              "         4.69902046e-02,  7.96279833e-02,  1.70439724e-02, -8.12049881e-02,\n",
              "        -4.15563583e-04,  3.82156372e-02,  6.52442947e-02,  1.67075153e-02,\n",
              "         4.87136841e-02,  3.01793106e-02, -1.13975383e-01,  2.79247276e-02,\n",
              "         4.60609421e-02,  6.29425049e-03, -9.04273987e-03,  3.80996689e-02,\n",
              "        -9.44839492e-02,  3.44169624e-02,  3.24434265e-02, -9.76371765e-03,\n",
              "         3.51305008e-02,  1.76750179e-02,  5.74109070e-02,  9.01439637e-02,\n",
              "         1.27042726e-01, -4.17344086e-02,  1.49835199e-01, -4.94567864e-02,\n",
              "        -5.36773689e-02, -6.70043975e-02,  1.08753201e-02,  2.86628716e-02,\n",
              "        -1.70671456e-02,  4.05403152e-02,  3.09402458e-02,  4.16473374e-02,\n",
              "        -8.67080688e-03, -1.24420170e-02,  1.27518894e-02,  6.47247285e-02,\n",
              "        -1.35162352e-02, -3.47866043e-02,  4.57191467e-02, -1.01640321e-01,\n",
              "        -2.61943825e-02,  4.70793732e-02, -3.53210457e-02, -7.39316940e-02,\n",
              "         4.23469534e-03, -3.88991348e-02, -6.32598847e-02,  8.84429961e-02,\n",
              "        -3.27739716e-02,  9.79995728e-02,  4.36126702e-02,  5.45843132e-02,\n",
              "         7.71232620e-02,  5.06515522e-03, -8.50608796e-02, -5.46974167e-02,\n",
              "         7.68501312e-02,  6.04309067e-02,  3.65425125e-02,  1.06798269e-01,\n",
              "        -7.77816749e-04, -3.74877937e-02,  5.59246056e-02, -3.46374512e-02,\n",
              "        -2.91671753e-02, -4.42085266e-02, -6.15497604e-02,  1.24188229e-01,\n",
              "         1.25877382e-02, -1.18584633e-02,  2.04723366e-02,  5.71105964e-02,\n",
              "        -4.41580787e-02, -6.64787292e-02, -2.77069099e-02, -4.51822281e-02,\n",
              "         1.79725643e-02,  3.21531296e-03,  2.08557136e-02,  2.16083531e-03,\n",
              "        -3.95177826e-02, -5.02502434e-02,  2.69790646e-02,  1.79237369e-02,\n",
              "        -9.65957623e-03, -8.34045410e-02, -5.47393784e-02, -3.02257538e-02,\n",
              "         5.55526726e-02, -6.58843964e-02, -2.43621822e-02, -5.74256666e-02,\n",
              "        -3.45718376e-02,  1.53861996e-02,  7.31920227e-02,  6.87017431e-03,\n",
              "         2.41783150e-02, -4.30267341e-02,  7.95543641e-02,  4.07592766e-02,\n",
              "        -4.86175530e-02, -1.53846736e-03, -5.67214973e-02,  3.54614258e-02,\n",
              "        -3.04656979e-02, -4.02679443e-02, -3.69080789e-02, -4.56015244e-02,\n",
              "        -9.47341882e-03, -2.32849121e-02,  8.15124530e-03, -4.62371819e-02,\n",
              "        -4.77100387e-02, -1.87765118e-02, -3.58537920e-02, -7.98706040e-02,\n",
              "        -1.21573806e-02,  2.99533848e-02, -3.68282311e-02,  4.89143357e-02,\n",
              "         7.68121257e-02, -1.04066469e-01,  1.11846924e-02,  2.81173699e-02,\n",
              "         1.88797005e-02,  1.31111145e-02, -3.63487229e-02, -7.26043731e-02,\n",
              "        -3.05137634e-02, -4.36157212e-02,  5.42915352e-02, -5.08651743e-03,\n",
              "        -7.71987885e-02,  1.34746553e-02, -8.35990906e-03, -2.25494392e-02,\n",
              "        -6.77558929e-02, -9.94918793e-02, -7.12554902e-02, -1.62429810e-02,\n",
              "        -7.75543228e-02,  5.85430153e-02,  3.96080017e-02,  1.06494902e-02,\n",
              "         1.57707222e-02, -7.95923248e-02,  5.16090393e-02, -2.39788052e-02,\n",
              "         2.24380493e-02, -1.67625435e-02, -1.43626779e-01, -6.27029389e-02,\n",
              "        -3.43208313e-02, -6.64203614e-02, -1.98883060e-02, -5.16620651e-02,\n",
              "         4.89952080e-02, -8.91160965e-02,  1.19705200e-02,  3.21736336e-02,\n",
              "        -7.78085738e-02, -6.09596260e-02, -7.12280255e-03,  1.57897957e-02,\n",
              "        -1.80313103e-02,  1.24359131e-03, -1.28534315e-02,  5.72589859e-02,\n",
              "         7.70374313e-02,  5.26544563e-02,  4.32167053e-02,  4.45182808e-02,\n",
              "         5.21606430e-02,  1.00330357e-02, -5.71876541e-02, -9.94105358e-03,\n",
              "        -4.26256172e-02,  2.76298523e-02, -7.23762531e-03, -9.08630341e-02,\n",
              "        -3.67277153e-02,  7.80853257e-02, -1.98280569e-02, -7.97596015e-03,\n",
              "        -6.13522530e-03, -3.29081528e-02, -5.74646005e-03, -2.16148384e-02,\n",
              "         1.99504849e-02, -2.60086060e-02, -1.61930081e-02,  5.63492775e-02,\n",
              "        -1.27555849e-02,  1.66149139e-02, -1.19348146e-01, -1.69197079e-02,\n",
              "         7.14569092e-02, -3.24094780e-02, -1.25898749e-01,  1.64230578e-02,\n",
              "        -1.84694286e-02, -4.23339829e-02, -3.54293808e-02,  9.97085590e-03,\n",
              "         7.57110566e-02, -6.84967041e-02,  8.11614990e-02,  5.84793091e-02,\n",
              "         4.50592041e-02, -3.08341975e-03,  4.66690063e-02, -4.34539802e-02,\n",
              "         3.76739493e-03,  2.77647981e-03,  6.44153580e-02,  1.37474062e-02,\n",
              "         3.21044913e-03, -7.02060685e-02,  7.32808113e-02,  5.21697989e-03,\n",
              "         3.89554985e-02,  1.51580814e-02,  3.13934311e-02, -5.33218384e-02,\n",
              "        -1.15455631e-02, -1.23214719e-04,  1.10153202e-02,  7.69004822e-02,\n",
              "        -1.27731320e-02, -2.17285156e-02, -2.35122684e-02,  1.16636278e-02,\n",
              "         2.24884041e-02,  7.03315735e-02,  6.18972778e-02, -3.65036018e-02,\n",
              "         3.30268852e-02,  5.79879768e-02, -2.20840462e-02, -8.74282867e-02,\n",
              "         1.64566038e-03, -4.93575111e-02, -4.11090851e-02,  7.87521377e-02,\n",
              "         5.97877502e-02,  9.46735367e-02, -3.79089341e-02, -1.92199703e-02,\n",
              "        -5.47834411e-02, -3.13152298e-02,  3.44192497e-02,  1.16050720e-01,\n",
              "         1.00509644e-01,  2.32593529e-02, -7.31277466e-03, -7.69866928e-02,\n",
              "        -2.83645634e-02, -1.00241087e-01, -1.65527351e-02,  1.81449894e-02,\n",
              "         4.40757759e-02, -1.26106264e-02,  1.67160039e-03,  7.74726868e-02,\n",
              "         4.50897205e-04, -1.17492676e-03, -6.85501099e-02, -3.19103226e-02,\n",
              "         2.58888248e-02,  2.29614265e-02, -4.82833870e-02,  6.15737922e-02,\n",
              "        -6.28875718e-02,  1.82895660e-02, -2.62901299e-02, -4.15069573e-02,\n",
              "        -1.18621830e-02, -5.05264290e-02,  4.97327819e-02, -9.73472558e-03],\n",
              "       dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "caba5200",
      "metadata": {
        "collapsed": true,
        "id": "caba5200",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da493a43-fb18-4a78-977a-e6cf52e21705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2088\n",
            "nan\n",
            "2420\n",
            "nan\n",
            "2961\n",
            "nan\n",
            "3536\n",
            "nan\n",
            "3798\n",
            "nan\n",
            "4738\n",
            "nan\n",
            "4739\n",
            "nan\n",
            "4985\n",
            "nan\n",
            "5114\n",
            "nan\n",
            "5123\n",
            "nan\n",
            "5511\n",
            "nan\n",
            "5513\n",
            "nan\n",
            "5972\n",
            "nan\n",
            "32\n",
            "nan\n",
            "3142\n",
            "nan\n",
            "3163\n",
            "nan\n",
            "3584\n",
            "nan\n",
            "3832\n",
            "nan\n",
            "5356\n",
            "nan\n",
            "6481\n",
            "nan\n",
            "8596\n",
            "nan\n",
            "8639\n",
            "nan\n",
            "9802\n",
            "nan\n",
            "9907\n",
            "nan\n",
            "11492\n",
            "nan\n",
            "11502\n",
            "nan\n",
            "11876\n",
            "nan\n",
            "12570\n",
            "nan\n",
            "13443\n",
            "nan\n",
            "13644\n",
            "nan\n",
            "14035\n",
            "nan\n",
            "14200\n",
            "nan\n",
            "14769\n",
            "nan\n",
            "15572\n",
            "nan\n",
            "16747\n",
            "nan\n",
            "18189\n",
            "nan\n",
            "18430\n",
            "nan\n",
            "18431\n",
            "nan\n",
            "18913\n",
            "nan\n",
            "18920\n",
            "nan\n",
            "18922\n",
            "nan\n",
            "18929\n",
            "nan\n",
            "18940\n",
            "nan\n",
            "18976\n",
            "nan\n",
            "19018\n",
            "nan\n",
            "19020\n",
            "nan\n",
            "19096\n",
            "nan\n",
            "19120\n",
            "nan\n",
            "19194\n",
            "nan\n",
            "19209\n",
            "nan\n",
            "19288\n",
            "nan\n",
            "19387\n",
            "nan\n",
            "19526\n",
            "nan\n",
            "19534\n",
            "nan\n",
            "19539\n",
            "nan\n",
            "19573\n",
            "nan\n",
            "19633\n",
            "nan\n",
            "19642\n",
            "nan\n",
            "19657\n",
            "nan\n",
            "19671\n",
            "nan\n",
            "19714\n",
            "nan\n",
            "19756\n",
            "nan\n",
            "19806\n",
            "nan\n",
            "19811\n",
            "nan\n",
            "20234\n",
            "nan\n",
            "20330\n",
            "nan\n",
            "20426\n",
            "nan\n",
            "20486\n",
            "nan\n",
            "20568\n",
            "nan\n",
            "20738\n",
            "nan\n",
            "20981\n",
            "nan\n",
            "21317\n",
            "nan\n",
            "21681\n",
            "nan\n",
            "21777\n",
            "nan\n",
            "nan\n",
            "nan\n"
          ]
        }
      ],
      "source": [
        "#non word vector values are removed \n",
        "wv_train = []\n",
        "for i, x in enumerate(X_train1):\n",
        "    try:\n",
        "        len(x)\n",
        "        wv_train.append(i)\n",
        "    except:\n",
        "        print(i)\n",
        "        print(x)\n",
        "        \n",
        "wv_unlabeled = []\n",
        "for i, x in enumerate(X_unlabeled1):\n",
        "    try:\n",
        "        len(x)\n",
        "        wv_unlabeled.append(i)\n",
        "    except:\n",
        "        print(i)\n",
        "        print(x)\n",
        "\n",
        "\n",
        "wv_test = []\n",
        "for i, x in enumerate(X_test1):\n",
        "    try:\n",
        "        len(x)\n",
        "        wv_test.append(i)\n",
        "    except:\n",
        "        print(x)\n",
        "X_train1 = [X_train1[i] for i in wv_train]\n",
        "X_test1 = [X_test1[i] for i in wv_test]\n",
        "X_unlabeled1 = [X_unlabeled1[i] for i in wv_unlabeled]\n",
        "y_train1 = [y_train[i] for i in wv_train]\n",
        "y_test1 = [y_test[i] for i in wv_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF VECTORIZATION"
      ],
      "metadata": {
        "id": "d9o_MSOVtTDc"
      },
      "id": "d9o_MSOVtTDc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "PERCEPTRON"
      ],
      "metadata": {
        "id": "bpyp1dfgtV5z"
      },
      "id": "bpyp1dfgtV5z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEURAL NETS"
      ],
      "metadata": {
        "id": "6jKFDveltaeg"
      },
      "id": "6jKFDveltaeg"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d9f72f54",
      "metadata": {
        "id": "d9f72f54",
        "outputId": "358b0717-1800-4db9-a9a0-28f4b4472c4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b69ca21d",
      "metadata": {
        "id": "b69ca21d"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "6ad25157",
      "metadata": {
        "id": "6ad25157"
      },
      "outputs": [],
      "source": [
        "# Enable CUDA\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "74430621",
      "metadata": {
        "id": "74430621"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    \n",
        "    def __init__(self, X, y):\n",
        "        self.X=torch.as_tensor(X)\n",
        "        self.y=torch.as_tensor(y)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        X_1=self.X[index]\n",
        "        y_1=self.y[index]\n",
        "        return X_1, y_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3257ccb7",
      "metadata": {
        "id": "3257ccb7"
      },
      "outputs": [],
      "source": [
        "#Training dataset split into training and validation set so we have a division of 60(training), 20(validation) and 20(testing)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_f, X_val_f, y_train_f, y_val_f = train_test_split(\n",
        "    X_train1, y_train1, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "62dfa175",
      "metadata": {
        "id": "62dfa175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4eef760-bbf8-4f12-eb7e-528ba892db53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-deb5d617ec55>:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  self.X=torch.as_tensor(X)\n"
          ]
        }
      ],
      "source": [
        "#genearting the training set, testing set and the validation set\n",
        "train_set = Dataset(X_train_f, y_train_f)\n",
        "valid_set = Dataset(X_val_f, y_val_f)\n",
        "test_set = Dataset(X_test1, y_test1)\n",
        "unlabeled_set = Dataset(X_unlabeled1, np.zeros(len(X_unlabeled1)))\n",
        "#Generating the data loaders for the training set, testing set and the validation set\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=0)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set,  batch_size=32, shuffle=True, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, shuffle=False)\n",
        "unlabeled_loader = torch.utils.data.DataLoader(unlabeled_set, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for x,y in unlabeled_loader:\n",
        "#   print(x,y)\n",
        "#   print(len(x))\n",
        "#   break"
      ],
      "metadata": {
        "id": "BLSuVtIcrkEj"
      },
      "id": "BLSuVtIcrkEj",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e3c4fc2f",
      "metadata": {
        "id": "e3c4fc2f"
      },
      "outputs": [],
      "source": [
        "#Generating the network architechture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_dim=300, output_dim=1, hidden_1=100, hidden_2=10):\n",
        "        super(Net, self).__init__()\n",
        "        self.input_dim = input_dim \n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_1 = hidden_1\n",
        "        self.hidden_2 = hidden_2\n",
        "        self.fc1 = nn.Linear(self.input_dim, self.hidden_1)\n",
        "        self.fc2 = nn.Linear(self.hidden_1, self.hidden_2)\n",
        "        self.fc3 = nn.Linear(self.hidden_2, self.output_dim)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "8bdbed36",
      "metadata": {
        "id": "8bdbed36",
        "outputId": "acf76ccd-6cac-4605-f6de-55f3a4f5c5b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Calling the Net is_depression to initialize the neural network\n",
        "model_te = Net()\n",
        "print(model_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "eaa0a97f",
      "metadata": {
        "id": "eaa0a97f"
      },
      "outputs": [],
      "source": [
        "#Defining the loss and the optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model_te.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_loss_min = np.Inf\n",
        "max_epochs=45\n",
        "for epoch in range(max_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    model_te.train()\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model_te(data)\n",
        "\n",
        "        #print(output)\n",
        "        #print(target)\n",
        "        target = target.unsqueeze(1)\n",
        "        target = target.float()\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "    model_te.eval()\n",
        "    for data, target in valid_loader:\n",
        "        output = model_te(data)\n",
        "        target = target.unsqueeze(1)\n",
        "        target = target.float()\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "    #Printing the loss values\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch+1, \n",
        "        train_loss,\n",
        "        valid_loss\n",
        "        ))\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model_te.state_dict(), 'model_te.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68YTTm3WG4mq",
        "outputId": "cd8e10f5-4faa-4ddf-82af-aa06dae99b1b"
      },
      "id": "68YTTm3WG4mq",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.694971 \tValidation Loss: 0.694970\n",
            "Validation loss decreased (inf --> 0.694970).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.693746 \tValidation Loss: 0.693933\n",
            "Validation loss decreased (0.694970 --> 0.693933).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.693162 \tValidation Loss: 0.693200\n",
            "Validation loss decreased (0.693933 --> 0.693200).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.692696 \tValidation Loss: 0.692830\n",
            "Validation loss decreased (0.693200 --> 0.692830).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.692553 \tValidation Loss: 0.692573\n",
            "Validation loss decreased (0.692830 --> 0.692573).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.692436 \tValidation Loss: 0.692394\n",
            "Validation loss decreased (0.692573 --> 0.692394).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.692277 \tValidation Loss: 0.692131\n",
            "Validation loss decreased (0.692394 --> 0.692131).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.692126 \tValidation Loss: 0.691906\n",
            "Validation loss decreased (0.692131 --> 0.691906).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.691848 \tValidation Loss: 0.691714\n",
            "Validation loss decreased (0.691906 --> 0.691714).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.691596 \tValidation Loss: 0.691492\n",
            "Validation loss decreased (0.691714 --> 0.691492).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.691551 \tValidation Loss: 0.691313\n",
            "Validation loss decreased (0.691492 --> 0.691313).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.691278 \tValidation Loss: 0.691131\n",
            "Validation loss decreased (0.691313 --> 0.691131).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 0.691221 \tValidation Loss: 0.690887\n",
            "Validation loss decreased (0.691131 --> 0.690887).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.690900 \tValidation Loss: 0.690606\n",
            "Validation loss decreased (0.690887 --> 0.690606).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.690304 \tValidation Loss: 0.690270\n",
            "Validation loss decreased (0.690606 --> 0.690270).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 0.690373 \tValidation Loss: 0.689956\n",
            "Validation loss decreased (0.690270 --> 0.689956).  Saving model ...\n",
            "Epoch: 17 \tTraining Loss: 0.690092 \tValidation Loss: 0.689689\n",
            "Validation loss decreased (0.689956 --> 0.689689).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.689620 \tValidation Loss: 0.689343\n",
            "Validation loss decreased (0.689689 --> 0.689343).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 0.689206 \tValidation Loss: 0.688896\n",
            "Validation loss decreased (0.689343 --> 0.688896).  Saving model ...\n",
            "Epoch: 20 \tTraining Loss: 0.688679 \tValidation Loss: 0.688349\n",
            "Validation loss decreased (0.688896 --> 0.688349).  Saving model ...\n",
            "Epoch: 21 \tTraining Loss: 0.688455 \tValidation Loss: 0.687715\n",
            "Validation loss decreased (0.688349 --> 0.687715).  Saving model ...\n",
            "Epoch: 22 \tTraining Loss: 0.687442 \tValidation Loss: 0.687111\n",
            "Validation loss decreased (0.687715 --> 0.687111).  Saving model ...\n",
            "Epoch: 23 \tTraining Loss: 0.687217 \tValidation Loss: 0.686430\n",
            "Validation loss decreased (0.687111 --> 0.686430).  Saving model ...\n",
            "Epoch: 24 \tTraining Loss: 0.686178 \tValidation Loss: 0.685671\n",
            "Validation loss decreased (0.686430 --> 0.685671).  Saving model ...\n",
            "Epoch: 25 \tTraining Loss: 0.685510 \tValidation Loss: 0.684716\n",
            "Validation loss decreased (0.685671 --> 0.684716).  Saving model ...\n",
            "Epoch: 26 \tTraining Loss: 0.684513 \tValidation Loss: 0.683720\n",
            "Validation loss decreased (0.684716 --> 0.683720).  Saving model ...\n",
            "Epoch: 27 \tTraining Loss: 0.683261 \tValidation Loss: 0.682585\n",
            "Validation loss decreased (0.683720 --> 0.682585).  Saving model ...\n",
            "Epoch: 28 \tTraining Loss: 0.682110 \tValidation Loss: 0.681140\n",
            "Validation loss decreased (0.682585 --> 0.681140).  Saving model ...\n",
            "Epoch: 29 \tTraining Loss: 0.680761 \tValidation Loss: 0.679662\n",
            "Validation loss decreased (0.681140 --> 0.679662).  Saving model ...\n",
            "Epoch: 30 \tTraining Loss: 0.678760 \tValidation Loss: 0.678020\n",
            "Validation loss decreased (0.679662 --> 0.678020).  Saving model ...\n",
            "Epoch: 31 \tTraining Loss: 0.677325 \tValidation Loss: 0.675862\n",
            "Validation loss decreased (0.678020 --> 0.675862).  Saving model ...\n",
            "Epoch: 32 \tTraining Loss: 0.675064 \tValidation Loss: 0.673402\n",
            "Validation loss decreased (0.675862 --> 0.673402).  Saving model ...\n",
            "Epoch: 33 \tTraining Loss: 0.672951 \tValidation Loss: 0.670512\n",
            "Validation loss decreased (0.673402 --> 0.670512).  Saving model ...\n",
            "Epoch: 34 \tTraining Loss: 0.669339 \tValidation Loss: 0.667312\n",
            "Validation loss decreased (0.670512 --> 0.667312).  Saving model ...\n",
            "Epoch: 35 \tTraining Loss: 0.665548 \tValidation Loss: 0.663428\n",
            "Validation loss decreased (0.667312 --> 0.663428).  Saving model ...\n",
            "Epoch: 36 \tTraining Loss: 0.660952 \tValidation Loss: 0.658610\n",
            "Validation loss decreased (0.663428 --> 0.658610).  Saving model ...\n",
            "Epoch: 37 \tTraining Loss: 0.656904 \tValidation Loss: 0.652847\n",
            "Validation loss decreased (0.658610 --> 0.652847).  Saving model ...\n",
            "Epoch: 38 \tTraining Loss: 0.649634 \tValidation Loss: 0.646260\n",
            "Validation loss decreased (0.652847 --> 0.646260).  Saving model ...\n",
            "Epoch: 39 \tTraining Loss: 0.643985 \tValidation Loss: 0.638260\n",
            "Validation loss decreased (0.646260 --> 0.638260).  Saving model ...\n",
            "Epoch: 40 \tTraining Loss: 0.635207 \tValidation Loss: 0.628952\n",
            "Validation loss decreased (0.638260 --> 0.628952).  Saving model ...\n",
            "Epoch: 41 \tTraining Loss: 0.626511 \tValidation Loss: 0.618299\n",
            "Validation loss decreased (0.628952 --> 0.618299).  Saving model ...\n",
            "Epoch: 42 \tTraining Loss: 0.613929 \tValidation Loss: 0.605937\n",
            "Validation loss decreased (0.618299 --> 0.605937).  Saving model ...\n",
            "Epoch: 43 \tTraining Loss: 0.601212 \tValidation Loss: 0.591244\n",
            "Validation loss decreased (0.605937 --> 0.591244).  Saving model ...\n",
            "Epoch: 44 \tTraining Loss: 0.587914 \tValidation Loss: 0.575072\n",
            "Validation loss decreased (0.591244 --> 0.575072).  Saving model ...\n",
            "Epoch: 45 \tTraining Loss: 0.570868 \tValidation Loss: 0.557191\n",
            "Validation loss decreased (0.575072 --> 0.557191).  Saving model ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to test the model\n",
        "def predict(model, dataloader):\n",
        "    prediction_list = []\n",
        "    truth_list = []\n",
        "    m = 0\n",
        "    cnt = 0\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        outputs = model(batch[0])\n",
        "        if(outputs[0] < 0.5):\n",
        "          predicted = 0\n",
        "        else:\n",
        "          predicted = 1\n",
        "        #print(batch[1], predicted)\n",
        "        if(int(batch[1].item()) == int(predicted)):\n",
        "          m+=1\n",
        "        cnt+=1\n",
        "        #_, predicted = torch.max(outputs.data, 1)\n",
        "        prediction_list.append(int(predicted))\n",
        "        truth_list.append(int(batch[1].item()))\n",
        "    #acc = m/cnt\n",
        "    return prediction_list, truth_list"
      ],
      "metadata": {
        "id": "tBItJNxYHICG"
      },
      "id": "tBItJNxYHICG",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#Loading the model and generating the predictions\n",
        "model_te.load_state_dict(torch.load('model_te.pt'))\n",
        "pred, true_val = predict(model_te, test_loader)\n",
        "print(classification_report(true_val, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm42taR5HL9b",
        "outputId": "fb632923-9bc5-4cfd-b392-b6103193221f"
      },
      "id": "qm42taR5HL9b",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.83      0.86       778\n",
            "           1       0.84      0.91      0.87       767\n",
            "\n",
            "    accuracy                           0.87      1545\n",
            "   macro avg       0.87      0.87      0.87      1545\n",
            "weighted avg       0.87      0.87      0.87      1545\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling the Net is_depression to initialize the neural network\n",
        "model_te = Net()\n",
        "print(model_te)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ha5cdBtHEH7",
        "outputId": "39101251-4538-4d96-f259-bf6ecd77a22c"
      },
      "id": "-ha5cdBtHEH7",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model_te.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "mF-mMHXpKmhO"
      },
      "id": "mF-mMHXpKmhO",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "cfe5aadb",
      "metadata": {
        "collapsed": true,
        "id": "cfe5aadb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef446b6-0091-407b-c31b-94c95c3d07b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.694589 \tValidation Loss: 0.692485\n",
            "Validation loss decreased (inf --> 0.692485).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.693043 \tValidation Loss: 0.691897\n",
            "Validation loss decreased (0.692485 --> 0.691897).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.692319 \tValidation Loss: 0.691673\n",
            "Validation loss decreased (0.691897 --> 0.691673).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.692028 \tValidation Loss: 0.691513\n",
            "Validation loss decreased (0.691673 --> 0.691513).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.691717 \tValidation Loss: 0.691353\n",
            "Validation loss decreased (0.691513 --> 0.691353).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.691591 \tValidation Loss: 0.691174\n",
            "Validation loss decreased (0.691353 --> 0.691174).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.691280 \tValidation Loss: 0.690942\n",
            "Validation loss decreased (0.691174 --> 0.690942).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.690958 \tValidation Loss: 0.690705\n",
            "Validation loss decreased (0.690942 --> 0.690705).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.690889 \tValidation Loss: 0.690457\n",
            "Validation loss decreased (0.690705 --> 0.690457).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.690467 \tValidation Loss: 0.690192\n",
            "Validation loss decreased (0.690457 --> 0.690192).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.690231 \tValidation Loss: 0.689888\n",
            "Validation loss decreased (0.690192 --> 0.689888).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.689930 \tValidation Loss: 0.689514\n",
            "Validation loss decreased (0.689888 --> 0.689514).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 0.689417 \tValidation Loss: 0.689165\n",
            "Validation loss decreased (0.689514 --> 0.689165).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.689056 \tValidation Loss: 0.688714\n",
            "Validation loss decreased (0.689165 --> 0.688714).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.688484 \tValidation Loss: 0.688174\n",
            "Validation loss decreased (0.688714 --> 0.688174).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 0.688516 \tValidation Loss: 0.687657\n",
            "Validation loss decreased (0.688174 --> 0.687657).  Saving model ...\n",
            "Epoch: 17 \tTraining Loss: 0.687856 \tValidation Loss: 0.687044\n",
            "Validation loss decreased (0.687657 --> 0.687044).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.687309 \tValidation Loss: 0.686436\n",
            "Validation loss decreased (0.687044 --> 0.686436).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 0.686389 \tValidation Loss: 0.685663\n",
            "Validation loss decreased (0.686436 --> 0.685663).  Saving model ...\n",
            "Epoch: 20 \tTraining Loss: 0.685495 \tValidation Loss: 0.684829\n",
            "Validation loss decreased (0.685663 --> 0.684829).  Saving model ...\n",
            "Epoch: 21 \tTraining Loss: 0.684449 \tValidation Loss: 0.683834\n",
            "Validation loss decreased (0.684829 --> 0.683834).  Saving model ...\n",
            "Epoch: 22 \tTraining Loss: 0.683451 \tValidation Loss: 0.681818\n",
            "Validation loss decreased (0.683834 --> 0.681818).  Saving model ...\n",
            "Epoch: 23 \tTraining Loss: 0.681328 \tValidation Loss: 0.680052\n",
            "Validation loss decreased (0.681818 --> 0.680052).  Saving model ...\n",
            "Epoch: 24 \tTraining Loss: 0.679849 \tValidation Loss: 0.678353\n",
            "Validation loss decreased (0.680052 --> 0.678353).  Saving model ...\n",
            "Epoch: 25 \tTraining Loss: 0.677827 \tValidation Loss: 0.676228\n",
            "Validation loss decreased (0.678353 --> 0.676228).  Saving model ...\n",
            "Epoch: 26 \tTraining Loss: 0.675751 \tValidation Loss: 0.673902\n",
            "Validation loss decreased (0.676228 --> 0.673902).  Saving model ...\n",
            "Epoch: 27 \tTraining Loss: 0.673401 \tValidation Loss: 0.671136\n",
            "Validation loss decreased (0.673902 --> 0.671136).  Saving model ...\n",
            "Epoch: 28 \tTraining Loss: 0.670379 \tValidation Loss: 0.668081\n",
            "Validation loss decreased (0.671136 --> 0.668081).  Saving model ...\n",
            "Epoch: 29 \tTraining Loss: 0.665899 \tValidation Loss: 0.663807\n",
            "Validation loss decreased (0.668081 --> 0.663807).  Saving model ...\n",
            "Epoch: 30 \tTraining Loss: 0.662647 \tValidation Loss: 0.659167\n",
            "Validation loss decreased (0.663807 --> 0.659167).  Saving model ...\n",
            "Epoch: 31 \tTraining Loss: 0.658343 \tValidation Loss: 0.653767\n",
            "Validation loss decreased (0.659167 --> 0.653767).  Saving model ...\n",
            "Epoch: 32 \tTraining Loss: 0.651641 \tValidation Loss: 0.647090\n",
            "Validation loss decreased (0.653767 --> 0.647090).  Saving model ...\n",
            "Epoch: 33 \tTraining Loss: 0.643963 \tValidation Loss: 0.639067\n",
            "Validation loss decreased (0.647090 --> 0.639067).  Saving model ...\n",
            "Epoch: 34 \tTraining Loss: 0.637448 \tValidation Loss: 0.630419\n",
            "Validation loss decreased (0.639067 --> 0.630419).  Saving model ...\n",
            "Epoch: 35 \tTraining Loss: 0.626364 \tValidation Loss: 0.619791\n",
            "Validation loss decreased (0.630419 --> 0.619791).  Saving model ...\n",
            "Epoch: 36 \tTraining Loss: 0.617073 \tValidation Loss: 0.607587\n",
            "Validation loss decreased (0.619791 --> 0.607587).  Saving model ...\n",
            "Epoch: 37 \tTraining Loss: 0.604609 \tValidation Loss: 0.593256\n",
            "Validation loss decreased (0.607587 --> 0.593256).  Saving model ...\n",
            "Epoch: 38 \tTraining Loss: 0.591071 \tValidation Loss: 0.577350\n",
            "Validation loss decreased (0.593256 --> 0.577350).  Saving model ...\n",
            "Epoch: 39 \tTraining Loss: 0.575578 \tValidation Loss: 0.560450\n",
            "Validation loss decreased (0.577350 --> 0.560450).  Saving model ...\n",
            "Epoch: 40 \tTraining Loss: 0.557872 \tValidation Loss: 0.543284\n",
            "Validation loss decreased (0.560450 --> 0.543284).  Saving model ...\n",
            "Epoch: 41 \tTraining Loss: 0.541670 \tValidation Loss: 0.521985\n",
            "Validation loss decreased (0.543284 --> 0.521985).  Saving model ...\n",
            "Epoch: 42 \tTraining Loss: 0.521111 \tValidation Loss: 0.502159\n",
            "Validation loss decreased (0.521985 --> 0.502159).  Saving model ...\n",
            "Epoch: 43 \tTraining Loss: 0.505280 \tValidation Loss: 0.481714\n",
            "Validation loss decreased (0.502159 --> 0.481714).  Saving model ...\n",
            "Epoch: 44 \tTraining Loss: 0.487274 \tValidation Loss: 0.462607\n",
            "Validation loss decreased (0.481714 --> 0.462607).  Saving model ...\n",
            "Epoch: 45 \tTraining Loss: 0.470371 \tValidation Loss: 0.451300\n",
            "Validation loss decreased (0.462607 --> 0.451300).  Saving model ...\n",
            "4936 4936 21745\n",
            "5128 5128 21553\n",
            "Epoch: 1 \tTraining Loss: 0.441933 \tValidation Loss: 0.425870\n",
            "Validation loss decreased (0.451300 --> 0.425870).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.426303 \tValidation Loss: 0.410184\n",
            "Validation loss decreased (0.425870 --> 0.410184).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.410679 \tValidation Loss: 0.395303\n",
            "Validation loss decreased (0.410184 --> 0.395303).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.393874 \tValidation Loss: 0.388584\n",
            "Validation loss decreased (0.395303 --> 0.388584).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.380137 \tValidation Loss: 0.371099\n",
            "Validation loss decreased (0.388584 --> 0.371099).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.368746 \tValidation Loss: 0.361305\n",
            "Validation loss decreased (0.371099 --> 0.361305).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.362311 \tValidation Loss: 0.352550\n",
            "Validation loss decreased (0.361305 --> 0.352550).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.350685 \tValidation Loss: 0.351201\n",
            "Validation loss decreased (0.352550 --> 0.351201).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.352491 \tValidation Loss: 0.336344\n",
            "Validation loss decreased (0.351201 --> 0.336344).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.336791 \tValidation Loss: 0.329511\n",
            "Validation loss decreased (0.336344 --> 0.329511).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.335495 \tValidation Loss: 0.323235\n",
            "Validation loss decreased (0.329511 --> 0.323235).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.328734 \tValidation Loss: 0.318275\n",
            "Validation loss decreased (0.323235 --> 0.318275).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 0.318927 \tValidation Loss: 0.313189\n",
            "Validation loss decreased (0.318275 --> 0.313189).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.314914 \tValidation Loss: 0.315305\n",
            "Epoch: 15 \tTraining Loss: 0.313973 \tValidation Loss: 0.305448\n",
            "Validation loss decreased (0.313189 --> 0.305448).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 0.311113 \tValidation Loss: 0.301659\n",
            "Validation loss decreased (0.305448 --> 0.301659).  Saving model ...\n",
            "Epoch: 17 \tTraining Loss: 0.303104 \tValidation Loss: 0.298564\n",
            "Validation loss decreased (0.301659 --> 0.298564).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.303088 \tValidation Loss: 0.295133\n",
            "Validation loss decreased (0.298564 --> 0.295133).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 0.296072 \tValidation Loss: 0.292838\n",
            "Validation loss decreased (0.295133 --> 0.292838).  Saving model ...\n",
            "Epoch: 20 \tTraining Loss: 0.289026 \tValidation Loss: 0.290490\n",
            "Validation loss decreased (0.292838 --> 0.290490).  Saving model ...\n",
            "Epoch: 21 \tTraining Loss: 0.285933 \tValidation Loss: 0.294163\n",
            "Epoch: 22 \tTraining Loss: 0.279550 \tValidation Loss: 0.284153\n",
            "Validation loss decreased (0.290490 --> 0.284153).  Saving model ...\n",
            "Epoch: 23 \tTraining Loss: 0.282217 \tValidation Loss: 0.284563\n",
            "Epoch: 24 \tTraining Loss: 0.277827 \tValidation Loss: 0.284390\n",
            "Epoch: 25 \tTraining Loss: 0.281004 \tValidation Loss: 0.277456\n",
            "Validation loss decreased (0.284153 --> 0.277456).  Saving model ...\n",
            "Epoch: 26 \tTraining Loss: 0.271321 \tValidation Loss: 0.282729\n",
            "Epoch: 27 \tTraining Loss: 0.266700 \tValidation Loss: 0.273514\n",
            "Validation loss decreased (0.277456 --> 0.273514).  Saving model ...\n",
            "Epoch: 28 \tTraining Loss: 0.267359 \tValidation Loss: 0.271247\n",
            "Validation loss decreased (0.273514 --> 0.271247).  Saving model ...\n",
            "Epoch: 29 \tTraining Loss: 0.263201 \tValidation Loss: 0.279280\n",
            "Epoch: 30 \tTraining Loss: 0.263735 \tValidation Loss: 0.268986\n",
            "Validation loss decreased (0.271247 --> 0.268986).  Saving model ...\n",
            "Epoch: 31 \tTraining Loss: 0.261960 \tValidation Loss: 0.266277\n",
            "Validation loss decreased (0.268986 --> 0.266277).  Saving model ...\n",
            "Epoch: 32 \tTraining Loss: 0.254864 \tValidation Loss: 0.264689\n",
            "Validation loss decreased (0.266277 --> 0.264689).  Saving model ...\n",
            "Epoch: 33 \tTraining Loss: 0.262577 \tValidation Loss: 0.265289\n",
            "Epoch: 34 \tTraining Loss: 0.255678 \tValidation Loss: 0.263860\n",
            "Validation loss decreased (0.264689 --> 0.263860).  Saving model ...\n",
            "Epoch: 35 \tTraining Loss: 0.250101 \tValidation Loss: 0.275633\n",
            "Epoch: 36 \tTraining Loss: 0.248553 \tValidation Loss: 0.264502\n",
            "Epoch: 37 \tTraining Loss: 0.243791 \tValidation Loss: 0.266371\n",
            "Epoch: 38 \tTraining Loss: 0.241658 \tValidation Loss: 0.256416\n",
            "Validation loss decreased (0.263860 --> 0.256416).  Saving model ...\n",
            "Epoch: 39 \tTraining Loss: 0.248139 \tValidation Loss: 0.256767\n",
            "Epoch: 40 \tTraining Loss: 0.246408 \tValidation Loss: 0.254504\n",
            "Validation loss decreased (0.256416 --> 0.254504).  Saving model ...\n",
            "Epoch: 41 \tTraining Loss: 0.239165 \tValidation Loss: 0.258103\n",
            "Epoch: 42 \tTraining Loss: 0.238058 \tValidation Loss: 0.253049\n",
            "Validation loss decreased (0.254504 --> 0.253049).  Saving model ...\n",
            "Epoch: 43 \tTraining Loss: 0.235109 \tValidation Loss: 0.252263\n",
            "Validation loss decreased (0.253049 --> 0.252263).  Saving model ...\n",
            "Epoch: 44 \tTraining Loss: 0.235102 \tValidation Loss: 0.250464\n",
            "Validation loss decreased (0.252263 --> 0.250464).  Saving model ...\n",
            "Epoch: 45 \tTraining Loss: 0.235381 \tValidation Loss: 0.252334\n",
            "5128 5128 21553\n",
            "14423 14423 12258\n",
            "Epoch: 1 \tTraining Loss: 0.127536 \tValidation Loss: 0.261975\n",
            "Epoch: 2 \tTraining Loss: 0.124440 \tValidation Loss: 0.255931\n",
            "Epoch: 3 \tTraining Loss: 0.123435 \tValidation Loss: 0.255393\n",
            "Epoch: 4 \tTraining Loss: 0.117368 \tValidation Loss: 0.257476\n",
            "Epoch: 5 \tTraining Loss: 0.115851 \tValidation Loss: 0.261114\n",
            "Epoch: 6 \tTraining Loss: 0.111808 \tValidation Loss: 0.258937\n",
            "Epoch: 7 \tTraining Loss: 0.109608 \tValidation Loss: 0.269136\n",
            "Epoch: 8 \tTraining Loss: 0.111793 \tValidation Loss: 0.261587\n",
            "Epoch: 9 \tTraining Loss: 0.109214 \tValidation Loss: 0.263119\n",
            "Epoch: 10 \tTraining Loss: 0.106529 \tValidation Loss: 0.263432\n",
            "Epoch: 11 \tTraining Loss: 0.104440 \tValidation Loss: 0.260683\n",
            "Epoch: 12 \tTraining Loss: 0.104095 \tValidation Loss: 0.258543\n",
            "Epoch: 13 \tTraining Loss: 0.101472 \tValidation Loss: 0.261378\n",
            "Epoch: 14 \tTraining Loss: 0.102285 \tValidation Loss: 0.261818\n",
            "Epoch: 15 \tTraining Loss: 0.100424 \tValidation Loss: 0.257889\n",
            "Epoch: 16 \tTraining Loss: 0.099386 \tValidation Loss: 0.258718\n",
            "Epoch: 17 \tTraining Loss: 0.097830 \tValidation Loss: 0.267066\n",
            "Epoch: 18 \tTraining Loss: 0.096114 \tValidation Loss: 0.259946\n",
            "Epoch: 19 \tTraining Loss: 0.099613 \tValidation Loss: 0.256581\n",
            "Epoch: 20 \tTraining Loss: 0.095562 \tValidation Loss: 0.262051\n",
            "Epoch: 21 \tTraining Loss: 0.093741 \tValidation Loss: 0.258527\n",
            "Epoch: 22 \tTraining Loss: 0.092491 \tValidation Loss: 0.263762\n",
            "Epoch: 23 \tTraining Loss: 0.089720 \tValidation Loss: 0.260027\n",
            "Epoch: 24 \tTraining Loss: 0.094503 \tValidation Loss: 0.258155\n",
            "Epoch: 25 \tTraining Loss: 0.091416 \tValidation Loss: 0.256997\n",
            "Epoch: 26 \tTraining Loss: 0.089959 \tValidation Loss: 0.256959\n",
            "Epoch: 27 \tTraining Loss: 0.088543 \tValidation Loss: 0.256510\n",
            "Epoch: 28 \tTraining Loss: 0.089568 \tValidation Loss: 0.256969\n",
            "Epoch: 29 \tTraining Loss: 0.089492 \tValidation Loss: 0.254491\n",
            "Epoch: 30 \tTraining Loss: 0.086356 \tValidation Loss: 0.264313\n",
            "Epoch: 31 \tTraining Loss: 0.086973 \tValidation Loss: 0.265582\n",
            "Epoch: 32 \tTraining Loss: 0.084715 \tValidation Loss: 0.276833\n",
            "Epoch: 33 \tTraining Loss: 0.087178 \tValidation Loss: 0.261223\n",
            "Epoch: 34 \tTraining Loss: 0.083981 \tValidation Loss: 0.262306\n",
            "Epoch: 35 \tTraining Loss: 0.082095 \tValidation Loss: 0.256686\n",
            "Epoch: 36 \tTraining Loss: 0.084038 \tValidation Loss: 0.256781\n",
            "Epoch: 37 \tTraining Loss: 0.081932 \tValidation Loss: 0.258594\n",
            "Epoch: 38 \tTraining Loss: 0.081244 \tValidation Loss: 0.256942\n",
            "Epoch: 39 \tTraining Loss: 0.079877 \tValidation Loss: 0.257185\n",
            "Epoch: 40 \tTraining Loss: 0.080084 \tValidation Loss: 0.258056\n",
            "Epoch: 41 \tTraining Loss: 0.081247 \tValidation Loss: 0.255829\n",
            "Epoch: 42 \tTraining Loss: 0.079387 \tValidation Loss: 0.257965\n",
            "Epoch: 43 \tTraining Loss: 0.078683 \tValidation Loss: 0.262185\n",
            "Epoch: 44 \tTraining Loss: 0.078985 \tValidation Loss: 0.253500\n",
            "Epoch: 45 \tTraining Loss: 0.074364 \tValidation Loss: 0.258057\n",
            "14423 14423 12258\n",
            "14423 14423 12258\n"
          ]
        }
      ],
      "source": [
        "#traning and validating the model\n",
        "valid_loss_min = np.Inf\n",
        "max_epochs=45\n",
        "flag = 0\n",
        "while(flag == 0):\n",
        "  train_set = Dataset(X_train_f, y_train_f)\n",
        "  train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=0)\n",
        "  unlabeled_set = Dataset(X_unlabeled1, np.zeros(len(X_unlabeled1)))\n",
        "  unlabeled_loader = torch.utils.data.DataLoader(unlabeled_set, shuffle=False)\n",
        "  for epoch in range(max_epochs):\n",
        "      train_loss = 0.0\n",
        "      valid_loss = 0.0\n",
        "      model_te.train()\n",
        "      for data, target in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          output = model_te(data)\n",
        "\n",
        "          #print(output)\n",
        "          #print(target)\n",
        "          target = target.unsqueeze(1)\n",
        "          target = target.float()\n",
        "          loss = criterion(output, target)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_loss += loss.item()*data.size(0)\n",
        "      model_te.eval()\n",
        "      for data, target in valid_loader:\n",
        "          output = model_te(data)\n",
        "          target = target.unsqueeze(1)\n",
        "          target = target.float()\n",
        "          loss = criterion(output, target)\n",
        "          valid_loss += loss.item()*data.size(0)\n",
        "      train_loss = train_loss/len(train_loader.dataset)\n",
        "      valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "      #Printing the loss values\n",
        "      print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "          epoch+1, \n",
        "          train_loss,\n",
        "          valid_loss\n",
        "          ))\n",
        "      if valid_loss <= valid_loss_min:\n",
        "          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "          valid_loss_min,\n",
        "          valid_loss))\n",
        "          torch.save(model_te.state_dict(), 'model_te.pt')\n",
        "          valid_loss_min = valid_loss\n",
        "  \n",
        "  flag = 1\n",
        "  model_te.load_state_dict(torch.load('model_te.pt'))\n",
        "  k=0\n",
        "  products_list = ['laptop', 'printer', 'tablet', 'desk', 'chair']\n",
        "\n",
        "  df_unlabeled = pd.DataFrame(zip(X_unlabeled1), columns = ['text'])\n",
        "  drp = []\n",
        "  print(len(X_train_f),len(y_train_f), len(X_unlabeled1))\n",
        "  for i ,batch in enumerate(unlabeled_loader):\n",
        "    outputs = model_te(batch[0][0])\n",
        "    #print(len(batch[0][0]))\n",
        "    #print(outputs)\n",
        "    if(outputs[0]<= 0.1 or outputs[0] > 0.9):\n",
        "      flag = 0\n",
        "      X_train_f.append(batch[0][0])\n",
        "      if(outputs[0] < 0.5):\n",
        "        y_train_f.append(0)\n",
        "      else:\n",
        "        y_train_f.append(1)\n",
        "      drp.append(i)\n",
        "    \n",
        "        #X_train_f.append(batch)\n",
        "  \n",
        "    #print(outputs[1])\n",
        "    #break\n",
        "  df_unlabeled = df_unlabeled.drop(drp)\n",
        "  X_unlabeled1 = df_unlabeled['text'].values.tolist()\n",
        "  print(len(X_train_f),len(y_train_f), len(X_unlabeled1))\n",
        "  #print(X_unlabeled1[0])\n",
        "  #print(type(X_unlabeled1[0]))\n",
        "  #print(type(X_unlabeled1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "eb255a3c",
      "metadata": {
        "id": "eb255a3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0591361e-2ee9-41cc-8fa0-fae70f519176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.88      0.91       778\n",
            "           1       0.89      0.95      0.92       767\n",
            "\n",
            "    accuracy                           0.91      1545\n",
            "   macro avg       0.92      0.91      0.91      1545\n",
            "weighted avg       0.92      0.91      0.91      1545\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#Loading the model and generating the predictions\n",
        "model_te.load_state_dict(torch.load('model_te.pt'))\n",
        "pred, true_val = predict(model_te, test_loader)\n",
        "print(classification_report(true_val, pred))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}